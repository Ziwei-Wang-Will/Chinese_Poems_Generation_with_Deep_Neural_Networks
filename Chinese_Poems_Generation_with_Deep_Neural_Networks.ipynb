{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1r5gV29LLWJ"
   },
   "source": [
    "# Chinese_Poems_Generation_with_Deep_Neural_Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main tasks\n",
    "\n",
    "1. Preprocessing for language modeling data\n",
    "    * Clean noise\n",
    "    * Create a dictionary that maps words to unique IDs\n",
    "    * Convert words to ID\n",
    "    * Reshape sequences to unified lengths\n",
    "2. Constructed a seq2seq model using RNNetwork with LSTM\n",
    "    * Hyperparameters\n",
    "    * Define a training operation for an epoch\n",
    "    * Define an operation for printing test data\n",
    "    * Define training controller\n",
    "3. Training and evaluation\n",
    "    * Observe loss\n",
    "    * Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwjBYH6hgPRP"
   },
   "source": [
    "## Task: Language Modeling\n",
    "Constructed a sequence-to-sequence model using Recurrent Neural Network with Long\n",
    "Short-term Memory cells\n",
    "\n",
    "### Definition\n",
    "We input a sequence of words/characters to an RNN so that it can learn the probability distribution of the next word/character in the sequence given the history of previous characters. This will then allow us to generate text one unit at a time.\n",
    "\n",
    "We will use Full Tang poetry(全唐诗) as our training data, and try to generate new poems later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing for language modeling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnXFHqP7fmJw"
   },
   "source": [
    "### Clean noise\n",
    "\n",
    "There are all kinds of noise in text data. We need to check the correctness of the preprocessing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0bI7rsH2LU_L"
   },
   "source": [
    "The format of our input data is like this:\n",
    "\n",
    "`(optional title + \":\")poem`\n",
    "\n",
    "We will use only the poem part and not the title.\n",
    "\n",
    "However, some special cases like:\n",
    "\n",
    "* 河鱼未上冻，江蛰已闻雷。（见《纬略》）\n",
    "* □□□□□\n",
    "\n",
    "We need some preprocessing.\n",
    "\n",
    "* Remove title\n",
    "* Remove spaces\n",
    "* Remove empty symbols\n",
    "* Replace other symbols\n",
    "\n",
    "Finally, we will randomize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDxfxaNbE1Na"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "data_filename ='poetry.txt'\n",
    "poems = []\n",
    "with open(data_filename, \"r\") as in_file:\n",
    "  for line in in_file.readlines():\n",
    "    line = line.strip()\n",
    "    # find title if exists\n",
    "    if ':' in line:\n",
    "      line = line.split(':')\n",
    "    # some poems are empty\n",
    "    if len(line) == 2:\n",
    "      poem = line[1]\n",
    "    else:\n",
    "      continue\n",
    "    # discard if contains special symbols\n",
    "    if re.search(r'[(（《_□]', poem):\n",
    "      continue\n",
    "    # discard if too short or too long\n",
    "    if len(poem) < 5 or len(poem) > 40:\n",
    "      continue\n",
    "    # remove symbols\n",
    "    poem = re.sub(u'[，。]','',poem)\n",
    "    poems.append(poem)\n",
    "\n",
    "poems = np.random.permutation(poems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFjPmxWXlZJ2"
   },
   "source": [
    "We select 5 poems as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqbNdpZYO8ma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11098, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_train, poems_test = poems[:-5], poems[-5:]\n",
    "len(poems_train), len(poems_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb9em1KyfuvZ"
   },
   "source": [
    "### Word to ID\n",
    "\n",
    "As mentioned lst time, we can use the tokenizer in Keras to help us. We set\n",
    "```python\n",
    "Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "```\n",
    "to not limit the number of words in the dictionary, and use character as our unit.\n",
    "\n",
    "We also need to correct the dictionary because it starts from 1 and that will be a problem later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1zXYlrxPMPx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars: 4762\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "poem_tokenizer = Tokenizer(num_words=None, lower=False, char_level=True)\n",
    "# Create word to ID dictionary\n",
    "poem_tokenizer.fit_on_texts(poems)\n",
    "# Get dictionary\n",
    "word_index = poem_tokenizer.word_index\n",
    "\n",
    "# Note that ID starts from 1!!\n",
    "# We need to add special ID 0\n",
    "word_index[\"<PAD>\"] = 0\n",
    "# Create ID to word \n",
    "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
    "print(\"Number of unique chars: {}\".format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29iNZFMwKzB-"
   },
   "source": [
    "Again, always check if there is any strange symbols in the dictionary. Here we only print first and last parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Bqy83LtQWjA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> 0\n",
      "不 1\n",
      "人 2\n",
      "一 3\n",
      "山 4\n",
      "风 5\n",
      "无 6\n",
      "花 7\n",
      "来 8\n",
      "日 9\n",
      "春 10\n",
      "诡 4757\n",
      "邢 4758\n",
      "颉 4759\n",
      "颃 4760\n",
      "荍 4761\n"
     ]
    }
   ],
   "source": [
    "# sort word index by ID\n",
    "for (w,i) in sorted(word_index.items(), key=lambda w: w[1]):\n",
    "# print some words to check if there are errors!\n",
    "  if i > 10 and i < len(word_index)-5: continue\n",
    "  print(\"{} {}\".format(w,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPVYU_l3qUq_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154, 45, 69, 194, 89, 16, 61, 58, 83, 4, 7, 68, 135, 223, 21, 73, 625, 211, 33, 235, 133, 32, 425, 5, 13, 486, 29, 210]\n",
      "两行客泪愁中落万树山花雨后残君到扬州见桃叶为传风水渡江难\n"
     ]
    }
   ],
   "source": [
    "# Apply word to ID on training and test set\n",
    "poems_train = poem_tokenizer.texts_to_sequences(poems_train)\n",
    "poems_test = poem_tokenizer.texts_to_sequences(poems_test)\n",
    "# Check and see if there is any error\n",
    "print(poems_train[0])\n",
    "print(''.join([reverse_word_index[w] for w in poems_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPBqvnAKf0Me"
   },
   "source": [
    "### Prepare the data for input\n",
    "\n",
    "We flatten the input to a long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reJzVQlkLgs_"
   },
   "outputs": [],
   "source": [
    "# flatten to a long string of characters\n",
    "poems_train = [w for po in poems_train for w in po]\n",
    "\n",
    "# flatten to a long string of characters\n",
    "poems_test = [w for po in poems_test for w in po]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj9yNonAf49F"
   },
   "source": [
    "### Define an input object\n",
    "\n",
    "We need to put the input into batches.\n",
    "* Reshape input data into a rectangular matrix and crop remainders\n",
    "* Calculate shape of each batch\n",
    "* Generate batch with input and output = input shift by one time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXoDJKqbTYnz"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNVdwFpgmoeq"
   },
   "outputs": [],
   "source": [
    "class PoemInput(object):\n",
    "  def __init__(self, data, config, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.sources, self.targets = self.input_producer(\n",
    "        data, batch_size, num_steps, name=name)\n",
    "\n",
    "  def input_producer(self, raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"Reshape the poem data to form input and output.\n",
    "    This chunks the raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "    Args:\n",
    "      raw_data: a list of words\n",
    "      batch_size: int, the batch size.\n",
    "      num_steps: int, the sequence length.\n",
    "      name: the name of this operation (optional).\n",
    "    Returns:\n",
    "      A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "      of the tuple is the same data time-shifted to the right by one.\n",
    "    \"\"\"\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "    # get size of the 1-d tensor\n",
    "    data_len = tf.size(raw_data)\n",
    "    # calculate how many batches\n",
    "    batch_len = data_len // batch_size\n",
    "    # crop data that does not fit in a batch\n",
    "    data = tf.reshape(raw_data[0:batch_size*batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "    # calculate how many batches in an epoch\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    # make sure there is at least one batch\n",
    "    assertion = tf.assert_positive(epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    # start generating slices\n",
    "    # range_input_producer returns a sequence of IDs \n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = data[:, i*num_steps  :(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructed a seq2seq model using RNNetwork with LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6k4i6WJ8gFLl"
   },
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhBnCcTTUpLS"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "class Hparam(object):\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 10\n",
    "  vocab_size = len(word_index)\n",
    "  embedding_size = 100\n",
    "  hidden_size = 100\n",
    "  warmup_epochs = 2\n",
    "  num_epochs_to_train = 40\n",
    "  keep_prob = 0.8\n",
    "  lr_decay = 0.95\n",
    "  batch_size = 100\n",
    "\n",
    "config = Hparam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyOWLTIVgJWZ"
   },
   "source": [
    "### Construct model\n",
    "In this step, the entire model structure must be defined completely. Including\n",
    "* Input\n",
    "* Size of layers\n",
    "* Connection between layers\n",
    "* Variables in layers\n",
    "* Output\n",
    "* Loss\n",
    "* Operations that apply the gradients (optimizer)\n",
    "* Placeholder for feeding special values\n",
    "* Properties that can be read from outside\n",
    "\n",
    "Note that we will use CudnnLSTM to speed up our training if available. However, I will provide two versions of LSTM here in case you cannot find a machine with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.cudnn_rnn import CudnnLSTM\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\n",
    "# from tensorflow.nn import embedding_lookup, dropout\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Build our model\n",
    "class MyModel(object):\n",
    "  def __init__(self, is_training, config, input_):\n",
    "    self._is_training = is_training\n",
    "    self._input = input_\n",
    "    self._rnn_params = None\n",
    "    self._cell = None\n",
    "    self.batch_size = input_.batch_size\n",
    "    self.num_steps = input_.num_steps\n",
    "    rnn_size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "    embedding_size = config.embedding_size\n",
    "\n",
    "    # Embeddings can only exist on CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      embedding_weights = tf.get_variable(\"embedding\", \\\n",
    "                     [vocab_size, embedding_size])\n",
    "      embed_inputs = tf.nn.embedding_lookup(embedding_weights, input_.sources)\n",
    "\n",
    "    if is_training and config.keep_prob < 1.:\n",
    "      embed_inputs = tf.nn.dropout(embed_inputs, config.keep_prob)\n",
    "\n",
    "    # build RNN using CudnnLSTM\n",
    "    # output, _ = self._build_rnn(embed_inputs, config, is_training)\n",
    "    # build RNN using basic LSTM\n",
    "    output, _ = self._build_rnn_old_lstm(embed_inputs, config, is_training)\n",
    "\n",
    "    # Remember RNN output is [batch_size x time, rnnsize]\n",
    "    # Dense layer for projecting onto vocabulary size\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Reshape logits to be a 3-D tensor for sequence loss\n",
    "    logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "    self._logits = logits\n",
    "\n",
    "    # Use the contrib sequence loss and average over the batches\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        input_.targets,\n",
    "        tf.ones([self.batch_size, self.num_steps]),\n",
    "        average_across_timesteps=False,\n",
    "        average_across_batch=True)\n",
    "\n",
    "    # Update the cost\n",
    "    self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    # A variable to store learning rate\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "\n",
    "    # Calculate gradients\n",
    "    # Get a list of trainable variables\n",
    "    tvars = tf.trainable_variables()\n",
    "    # Get gradient and clip by norm\n",
    "    grads, _ = tf.clip_by_global_norm(\\\n",
    "                 tf.gradients(self._cost, tvars),\n",
    "                 config.max_grad_norm)\n",
    "    # Define an optimizer\n",
    "    # Note that the optimizer reads the value of learning rate from variable\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "    # Define an operation that actually applies the gradients\n",
    "    self._train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.train.get_or_create_global_step())\n",
    "    # A placeholder for feeding new learning rates\n",
    "    self._new_lr = tf.placeholder(\n",
    "         tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "    self._lr_update_op = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "# build RNN using CudnnLSTM    \n",
    "  def _build_rnn(self, inputs, config, is_training):\n",
    "    # RNN requires time-major\n",
    "    inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "    self._cell = CudnnLSTM(\n",
    "        num_layers=config.num_layers,\n",
    "        num_units=config.hidden_size,\n",
    "        )\n",
    "    self._cell.build(inputs.get_shape())\n",
    "    outputs, state = self._cell(inputs)\n",
    "    # Transpose from time-major to batch-major\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    # Reshape from [batch, time, rnnsize] to [batch x time, rnnsize]\n",
    "    # For computing softmax later\n",
    "    outputs = tf.reshape(outputs, [-1, config.hidden_size])\n",
    "    return outputs, state\n",
    "\n",
    "# build RNN using basic LSTM\n",
    "  def _build_rnn_old_lstm(self, inputs, config, is_training):\n",
    "    def make_cell():\n",
    "      cell = BasicLSTMCell(\n",
    "        config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n",
    "        reuse=not is_training)\n",
    "      if is_training and config.keep_prob < 1:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "            cell, output_keep_prob=config.keep_prob)\n",
    "      return cell\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "    self._initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "    state = self._initial_state\n",
    "    outputs = []\n",
    "    inputs = tf.unstack(inputs, num=self.num_steps, axis=1)\n",
    "    outputs, state = tf.nn.static_rnn(cell, inputs,\n",
    "                                      initial_state=self._initial_state)\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "    return output, state\n",
    "  \n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(self._lr_update_op, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "  @property\n",
    "  def input(self):\n",
    "    return self._input\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op\n",
    "\n",
    "  @property\n",
    "  def logits(self):\n",
    "    return self._logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EYipAzwgXMD"
   },
   "source": [
    "### Define a training operation for an epoch\n",
    "This procedure gets the output from the model for each batch.\n",
    "We need a dictionary with these keys:\n",
    "\n",
    "* \"cost\": Reads the propertie `model.cost` that we defined above. \n",
    "* \"do_op\": Perform operation `model.train_op` that applies gradients\n",
    "\n",
    "After running (calling `session.run()`), the same key will contain the return values.\n",
    "\n",
    "We can add any key in the dictionary that corresponds to `@property` in the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84z2L2sggp48"
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, do_op=None, verbose=False):\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  feed_to_model_dict = {\n",
    "      \"cost\": model.cost,\n",
    "  }\n",
    "  # if an operation is provided, put that in the feed\n",
    "  if do_op is not None:\n",
    "    feed_to_model_dict[\"do_op\"] = do_op\n",
    "\n",
    "  for step in range(model.input.epoch_size):\n",
    "    # use the session to run, feed the dictionary\n",
    "    s_out = session.run(feed_to_model_dict)\n",
    "    # The returned dictionary will contain the information we need\n",
    "    cost = s_out[\"cost\"]\n",
    "    # Accumulate cost\n",
    "    costs += cost\n",
    "    # Accumulate number of training steps\n",
    "    iters += model.input.num_steps\n",
    "    # Print loss periodically\n",
    "    if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "      print(\"%.0f%% ppl: %.3f, speed: %.0f char/sec\" %\n",
    "            (step * 100.0 / model.input.epoch_size, \\\n",
    "             np.exp(costs/iters), \\\n",
    "             iters * model.input.batch_size/(time.time() - start_time)))\n",
    "\n",
    "  return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3LA13TWgrBg"
   },
   "source": [
    "### Define an operation for printing test data\n",
    "In the end, we want to see the generated poems. We will modify the decode procedure from last week's codelab to show Chinese characters.\n",
    "\n",
    "Here, we will again use `feed_to_model_dict` to get the output from the last layer (`logits`). Since they represent probability over vocabulary, we need to calculate `argmax` to get the actual ID of the char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hAnuy6BgkO-"
   },
   "outputs": [],
   "source": [
    "def run_test(session, model):\n",
    "\n",
    "  def decode_text(text):\n",
    "    words = [reverse_word_index.get(i, \"<UNK>\") for i in text]\n",
    "    fixed_width_string = []\n",
    "    # limit max length = 5\n",
    "    for w_pos in range(len(words)):\n",
    "      fixed_width_string.append(words[w_pos])\n",
    "      if (w_pos+1) % 5 == 0:\n",
    "        fixed_width_string.append('\\n')\n",
    "    return ''.join(fixed_width_string)\n",
    "\n",
    "  feed_to_model_dict = {\n",
    "      \"logits\": model.logits,\n",
    "  }\n",
    "\n",
    "  vals = session.run(feed_to_model_dict)\n",
    "  logits = vals[\"logits\"]\n",
    "  # Transform to a list of IDs\n",
    "  logits = logits.squeeze().argmax(axis=-1).tolist()\n",
    "  print(decode_text(logits))\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qvm0wW2inUwk"
   },
   "source": [
    "### Main training controller\n",
    "Finally, we define a controller that:\n",
    "* Create the model for training\n",
    "* Create the model for testing, copying from the training model\n",
    "* Prepare the input data\n",
    "* Define what to log in the progress of training\n",
    "* Create a `session` that communicates with computation graph\n",
    "* Change learning rate optionally\n",
    "* Get test set results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKsN1qiVhdms"
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "\n",
    "  eval_config = Hparam()\n",
    "  eval_config.batch_size = 1\n",
    "  eval_config.num_steps = 20\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "      # Create input producer\n",
    "      train_input = PoemInput(poems_train, config, name=\"TrainInput\")\n",
    "      # Create the model instance\n",
    "      with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "        m = MyModel(is_training=True, config=config, input_=train_input)\n",
    "      # Add information to logs\n",
    "      tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "      tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "      # Create another input for test data\n",
    "      # Note that eval_config was set locally\n",
    "      test_input = PoemInput(poems_test, eval_config, name=\"TestInput\")\n",
    "      # Create another model but reuse the variables in the training model\n",
    "      with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "        mtest = MyModel(is_training=False, config=eval_config,\n",
    "                         input_=test_input)\n",
    "    # Hardware settings\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement=True)\n",
    "    # Create a session that controls the training process\n",
    "    # Also automatically logs and reports \n",
    "    # Note the `checkpoint_dir` setting\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=\"logs\", \\\n",
    "                                           config=config_proto, \\\n",
    "                                           log_step_count_steps=-1) as session:\n",
    "      for i in range(config.num_epochs_to_train):\n",
    "        # Calculate learning rate decay\n",
    "        lr_decay = config.lr_decay ** max(i + 1 - config.warmup_epochs, 0.0)\n",
    "        # Set learning rate\n",
    "        m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "        # Print new learning rate\n",
    "        print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        # Train one epoch and report loss\n",
    "        train_perplexity = run_epoch(session, m, do_op=m.train_op,\n",
    "                                     verbose=True)\n",
    "        print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "      \n",
    "      # End of training\n",
    "      # Evaluate test set performance\n",
    "      test_perplexity = run_epoch(session, mtest)\n",
    "      print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "      # Print some examples from test\n",
    "      run_test(session, mtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_6KfSfUnbC5"
   },
   "source": [
    "### Start training\n",
    "We can actually start training by calling the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3f2BoLtl9Yo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into logs/model.ckpt.\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "3% ppl: 4151.129, speed: 5099 char/sec\n",
      "13% ppl: 2142.248, speed: 6954 char/sec\n",
      "23% ppl: 1752.580, speed: 7343 char/sec\n",
      "33% ppl: 1568.996, speed: 7510 char/sec\n",
      "43% ppl: 1475.089, speed: 6582 char/sec\n",
      "52% ppl: 1407.886, speed: 6474 char/sec\n",
      "62% ppl: 1358.670, speed: 6329 char/sec\n",
      "72% ppl: 1326.180, speed: 6255 char/sec\n",
      "82% ppl: 1297.161, speed: 6256 char/sec\n",
      "92% ppl: 1275.113, speed: 6409 char/sec\n",
      "Epoch: 1 Train Perplexity: 1263.766\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "3% ppl: 1116.350, speed: 8352 char/sec\n",
      "13% ppl: 1093.228, speed: 8356 char/sec\n",
      "23% ppl: 1115.026, speed: 7806 char/sec\n",
      "33% ppl: 1113.856, speed: 7241 char/sec\n",
      "43% ppl: 1118.207, speed: 7041 char/sec\n",
      "52% ppl: 1114.903, speed: 7077 char/sec\n",
      "62% ppl: 1110.325, speed: 7089 char/sec\n",
      "72% ppl: 1109.783, speed: 6976 char/sec\n",
      "82% ppl: 1106.025, speed: 7055 char/sec\n",
      "92% ppl: 1103.474, speed: 7145 char/sec\n",
      "Epoch: 2 Train Perplexity: 1104.715\n",
      "Epoch: 3 Learning rate: 0.950\n",
      "3% ppl: 1096.282, speed: 7455 char/sec\n",
      "13% ppl: 1073.396, speed: 7542 char/sec\n",
      "23% ppl: 1095.401, speed: 7111 char/sec\n",
      "33% ppl: 1094.461, speed: 6979 char/sec\n",
      "43% ppl: 1098.109, speed: 6904 char/sec\n",
      "52% ppl: 1094.444, speed: 6725 char/sec\n",
      "62% ppl: 1089.223, speed: 6862 char/sec\n",
      "72% ppl: 1088.059, speed: 6620 char/sec\n",
      "82% ppl: 1083.036, speed: 6605 char/sec\n",
      "92% ppl: 1079.568, speed: 6433 char/sec\n",
      "Epoch: 3 Train Perplexity: 1079.801\n",
      "Epoch: 4 Learning rate: 0.902\n",
      "3% ppl: 1068.700, speed: 6092 char/sec\n",
      "13% ppl: 1041.100, speed: 6453 char/sec\n",
      "23% ppl: 1061.543, speed: 5257 char/sec\n",
      "33% ppl: 1059.202, speed: 5576 char/sec\n",
      "43% ppl: 1062.288, speed: 5487 char/sec\n",
      "52% ppl: 1058.311, speed: 5136 char/sec\n",
      "62% ppl: 1053.088, speed: 5071 char/sec\n",
      "72% ppl: 1051.021, speed: 5091 char/sec\n",
      "82% ppl: 1045.810, speed: 5229 char/sec\n",
      "92% ppl: 1042.139, speed: 5358 char/sec\n",
      "Epoch: 4 Train Perplexity: 1042.158\n",
      "Epoch: 5 Learning rate: 0.857\n",
      "3% ppl: 1026.134, speed: 7578 char/sec\n",
      "13% ppl: 1003.413, speed: 7652 char/sec\n",
      "23% ppl: 1020.826, speed: 7385 char/sec\n",
      "33% ppl: 1018.891, speed: 7296 char/sec\n",
      "43% ppl: 1021.054, speed: 7330 char/sec\n",
      "52% ppl: 1016.594, speed: 7207 char/sec\n",
      "62% ppl: 1010.682, speed: 6984 char/sec\n",
      "72% ppl: 1008.167, speed: 6688 char/sec\n",
      "82% ppl: 1003.221, speed: 6821 char/sec\n",
      "92% ppl: 999.950, speed: 6906 char/sec\n",
      "Epoch: 5 Train Perplexity: 999.574\n",
      "Epoch: 6 Learning rate: 0.815\n",
      "3% ppl: 986.598, speed: 8059 char/sec\n",
      "13% ppl: 967.580, speed: 8036 char/sec\n",
      "23% ppl: 984.029, speed: 8004 char/sec\n",
      "33% ppl: 982.165, speed: 8018 char/sec\n",
      "43% ppl: 983.878, speed: 7972 char/sec\n",
      "52% ppl: 979.366, speed: 7831 char/sec\n",
      "62% ppl: 973.488, speed: 7735 char/sec\n",
      "72% ppl: 971.381, speed: 7771 char/sec\n",
      "82% ppl: 966.752, speed: 7784 char/sec\n",
      "92% ppl: 963.556, speed: 7814 char/sec\n",
      "Epoch: 6 Train Perplexity: 962.931\n",
      "Epoch: 7 Learning rate: 0.774\n",
      "3% ppl: 955.462, speed: 8067 char/sec\n",
      "13% ppl: 934.441, speed: 8076 char/sec\n",
      "23% ppl: 949.757, speed: 7986 char/sec\n",
      "33% ppl: 948.281, speed: 7818 char/sec\n",
      "43% ppl: 949.096, speed: 7358 char/sec\n",
      "52% ppl: 943.313, speed: 6740 char/sec\n",
      "62% ppl: 936.519, speed: 6801 char/sec\n",
      "72% ppl: 933.497, speed: 6704 char/sec\n",
      "82% ppl: 928.755, speed: 6750 char/sec\n",
      "92% ppl: 924.803, speed: 6379 char/sec\n",
      "Epoch: 7 Train Perplexity: 923.532\n",
      "Epoch: 8 Learning rate: 0.735\n",
      "3% ppl: 902.245, speed: 5956 char/sec\n",
      "13% ppl: 886.771, speed: 6013 char/sec\n",
      "23% ppl: 900.567, speed: 6290 char/sec\n",
      "33% ppl: 899.039, speed: 6499 char/sec\n",
      "43% ppl: 898.610, speed: 6629 char/sec\n",
      "52% ppl: 892.097, speed: 6665 char/sec\n",
      "62% ppl: 883.996, speed: 6816 char/sec\n",
      "72% ppl: 879.900, speed: 6587 char/sec\n",
      "82% ppl: 874.354, speed: 6505 char/sec\n",
      "92% ppl: 870.466, speed: 6498 char/sec\n",
      "Epoch: 8 Train Perplexity: 868.630\n",
      "Epoch: 9 Learning rate: 0.698\n",
      "3% ppl: 847.448, speed: 6304 char/sec\n",
      "13% ppl: 825.935, speed: 7309 char/sec\n",
      "23% ppl: 840.367, speed: 7592 char/sec\n",
      "33% ppl: 838.505, speed: 6772 char/sec\n",
      "43% ppl: 838.221, speed: 6541 char/sec\n",
      "52% ppl: 831.549, speed: 6594 char/sec\n",
      "62% ppl: 823.900, speed: 6692 char/sec\n",
      "72% ppl: 820.093, speed: 6624 char/sec\n",
      "82% ppl: 815.120, speed: 6458 char/sec\n",
      "92% ppl: 811.523, speed: 6359 char/sec\n",
      "Epoch: 9 Train Perplexity: 809.897\n",
      "Epoch: 10 Learning rate: 0.663\n",
      "3% ppl: 793.075, speed: 6879 char/sec\n",
      "13% ppl: 773.948, speed: 6777 char/sec\n",
      "23% ppl: 786.705, speed: 6867 char/sec\n",
      "33% ppl: 785.109, speed: 6868 char/sec\n",
      "43% ppl: 784.715, speed: 6673 char/sec\n",
      "52% ppl: 778.645, speed: 5874 char/sec\n",
      "62% ppl: 771.479, speed: 5732 char/sec\n",
      "72% ppl: 767.786, speed: 5777 char/sec\n",
      "82% ppl: 763.444, speed: 5649 char/sec\n",
      "92% ppl: 759.886, speed: 5550 char/sec\n",
      "Epoch: 10 Train Perplexity: 758.768\n",
      "Epoch: 11 Learning rate: 0.630\n",
      "3% ppl: 741.588, speed: 5844 char/sec\n",
      "13% ppl: 725.136, speed: 6543 char/sec\n",
      "23% ppl: 737.490, speed: 6612 char/sec\n",
      "33% ppl: 735.451, speed: 5971 char/sec\n",
      "43% ppl: 734.982, speed: 6061 char/sec\n",
      "52% ppl: 729.671, speed: 6139 char/sec\n",
      "62% ppl: 723.196, speed: 6181 char/sec\n",
      "72% ppl: 719.490, speed: 6113 char/sec\n",
      "82% ppl: 715.251, speed: 6125 char/sec\n",
      "92% ppl: 712.268, speed: 5941 char/sec\n",
      "Epoch: 11 Train Perplexity: 710.869\n",
      "Epoch: 12 Learning rate: 0.599\n",
      "3% ppl: 693.474, speed: 6341 char/sec\n",
      "13% ppl: 679.493, speed: 5762 char/sec\n",
      "23% ppl: 689.721, speed: 5343 char/sec\n",
      "33% ppl: 688.730, speed: 5281 char/sec\n",
      "43% ppl: 687.597, speed: 5692 char/sec\n",
      "52% ppl: 682.390, speed: 5774 char/sec\n",
      "62% ppl: 676.677, speed: 5789 char/sec\n",
      "72% ppl: 673.110, speed: 5998 char/sec\n",
      "82% ppl: 669.349, speed: 6120 char/sec\n",
      "92% ppl: 666.659, speed: 6248 char/sec\n",
      "Epoch: 12 Train Perplexity: 665.696\n",
      "Epoch: 13 Learning rate: 0.569\n",
      "3% ppl: 652.442, speed: 5727 char/sec\n",
      "13% ppl: 639.353, speed: 5010 char/sec\n",
      "23% ppl: 650.410, speed: 4489 char/sec\n",
      "33% ppl: 649.070, speed: 4624 char/sec\n",
      "43% ppl: 648.637, speed: 5083 char/sec\n",
      "52% ppl: 644.305, speed: 5437 char/sec\n",
      "62% ppl: 639.641, speed: 5718 char/sec\n",
      "72% ppl: 636.599, speed: 5941 char/sec\n",
      "82% ppl: 633.507, speed: 6119 char/sec\n",
      "92% ppl: 631.198, speed: 6099 char/sec\n",
      "Epoch: 13 Train Perplexity: 630.498\n",
      "Epoch: 14 Learning rate: 0.540\n",
      "3% ppl: 624.221, speed: 7759 char/sec\n",
      "13% ppl: 610.618, speed: 5883 char/sec\n",
      "23% ppl: 621.486, speed: 5844 char/sec\n",
      "INFO:tensorflow:Saving checkpoints for 3802 into logs/model.ckpt.\n",
      "33% ppl: 620.088, speed: 5135 char/sec\n",
      "43% ppl: 620.821, speed: 5164 char/sec\n",
      "52% ppl: 616.617, speed: 5167 char/sec\n",
      "62% ppl: 612.185, speed: 5308 char/sec\n",
      "72% ppl: 609.374, speed: 5203 char/sec\n",
      "82% ppl: 606.323, speed: 5254 char/sec\n",
      "92% ppl: 604.416, speed: 5244 char/sec\n",
      "Epoch: 14 Train Perplexity: 604.138\n",
      "Epoch: 15 Learning rate: 0.513\n",
      "3% ppl: 597.181, speed: 6976 char/sec\n",
      "13% ppl: 587.370, speed: 6778 char/sec\n",
      "23% ppl: 598.504, speed: 6704 char/sec\n",
      "33% ppl: 596.595, speed: 6798 char/sec\n",
      "43% ppl: 597.162, speed: 6552 char/sec\n",
      "52% ppl: 593.108, speed: 6684 char/sec\n",
      "62% ppl: 588.793, speed: 6800 char/sec\n",
      "72% ppl: 586.154, speed: 6900 char/sec\n",
      "82% ppl: 583.732, speed: 6823 char/sec\n",
      "92% ppl: 582.135, speed: 6861 char/sec\n",
      "Epoch: 15 Train Perplexity: 581.784\n",
      "Epoch: 16 Learning rate: 0.488\n",
      "3% ppl: 577.242, speed: 7806 char/sec\n",
      "13% ppl: 565.539, speed: 7556 char/sec\n",
      "23% ppl: 576.416, speed: 7440 char/sec\n",
      "33% ppl: 575.471, speed: 7502 char/sec\n",
      "43% ppl: 576.007, speed: 7567 char/sec\n",
      "52% ppl: 572.442, speed: 7618 char/sec\n",
      "62% ppl: 569.039, speed: 7516 char/sec\n",
      "72% ppl: 566.455, speed: 7333 char/sec\n",
      "82% ppl: 563.932, speed: 7116 char/sec\n",
      "92% ppl: 562.251, speed: 6919 char/sec\n",
      "Epoch: 16 Train Perplexity: 561.940\n",
      "Epoch: 17 Learning rate: 0.463\n",
      "3% ppl: 560.086, speed: 4145 char/sec\n",
      "13% ppl: 549.695, speed: 5072 char/sec\n",
      "23% ppl: 559.829, speed: 5515 char/sec\n",
      "33% ppl: 557.747, speed: 5671 char/sec\n",
      "43% ppl: 558.577, speed: 5828 char/sec\n",
      "52% ppl: 555.380, speed: 6001 char/sec\n",
      "62% ppl: 551.891, speed: 5833 char/sec\n",
      "72% ppl: 549.549, speed: 5756 char/sec\n",
      "82% ppl: 546.870, speed: 5814 char/sec\n",
      "92% ppl: 545.308, speed: 5875 char/sec\n",
      "Epoch: 17 Train Perplexity: 545.073\n",
      "Epoch: 18 Learning rate: 0.440\n",
      "3% ppl: 545.982, speed: 4429 char/sec\n",
      "13% ppl: 534.631, speed: 4364 char/sec\n",
      "23% ppl: 543.948, speed: 4199 char/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33% ppl: 542.683, speed: 4287 char/sec\n",
      "43% ppl: 542.934, speed: 4285 char/sec\n",
      "52% ppl: 539.794, speed: 4392 char/sec\n",
      "62% ppl: 536.146, speed: 4513 char/sec\n",
      "72% ppl: 534.139, speed: 4603 char/sec\n",
      "82% ppl: 532.175, speed: 4654 char/sec\n",
      "92% ppl: 530.671, speed: 4712 char/sec\n",
      "Epoch: 18 Train Perplexity: 530.421\n",
      "Epoch: 19 Learning rate: 0.418\n",
      "3% ppl: 525.045, speed: 5378 char/sec\n",
      "13% ppl: 517.536, speed: 4555 char/sec\n",
      "23% ppl: 527.904, speed: 5100 char/sec\n",
      "33% ppl: 527.046, speed: 5496 char/sec\n",
      "43% ppl: 527.054, speed: 5585 char/sec\n",
      "52% ppl: 523.982, speed: 5433 char/sec\n",
      "62% ppl: 520.376, speed: 5569 char/sec\n",
      "72% ppl: 518.287, speed: 5593 char/sec\n",
      "82% ppl: 516.268, speed: 5612 char/sec\n",
      "92% ppl: 515.350, speed: 5520 char/sec\n",
      "Epoch: 19 Train Perplexity: 515.429\n",
      "Epoch: 20 Learning rate: 0.397\n",
      "3% ppl: 516.692, speed: 6805 char/sec\n",
      "13% ppl: 507.088, speed: 5692 char/sec\n",
      "23% ppl: 515.975, speed: 6244 char/sec\n",
      "33% ppl: 514.220, speed: 5892 char/sec\n",
      "43% ppl: 514.994, speed: 5840 char/sec\n",
      "52% ppl: 511.980, speed: 5930 char/sec\n",
      "62% ppl: 508.529, speed: 6118 char/sec\n",
      "72% ppl: 506.089, speed: 5952 char/sec\n",
      "82% ppl: 503.890, speed: 6048 char/sec\n",
      "92% ppl: 502.697, speed: 5975 char/sec\n",
      "Epoch: 20 Train Perplexity: 502.636\n",
      "Epoch: 21 Learning rate: 0.377\n",
      "3% ppl: 505.614, speed: 5630 char/sec\n",
      "13% ppl: 493.457, speed: 6836 char/sec\n",
      "23% ppl: 502.185, speed: 5967 char/sec\n",
      "33% ppl: 501.117, speed: 4907 char/sec\n",
      "43% ppl: 501.979, speed: 4977 char/sec\n",
      "52% ppl: 499.380, speed: 4856 char/sec\n",
      "62% ppl: 496.206, speed: 4835 char/sec\n",
      "72% ppl: 494.512, speed: 4970 char/sec\n",
      "82% ppl: 492.528, speed: 5068 char/sec\n",
      "92% ppl: 491.483, speed: 5130 char/sec\n",
      "Epoch: 21 Train Perplexity: 491.232\n",
      "Epoch: 22 Learning rate: 0.358\n",
      "3% ppl: 492.736, speed: 4475 char/sec\n",
      "13% ppl: 483.789, speed: 4921 char/sec\n",
      "23% ppl: 492.001, speed: 5582 char/sec\n",
      "33% ppl: 489.800, speed: 5978 char/sec\n",
      "43% ppl: 490.899, speed: 6170 char/sec\n",
      "52% ppl: 487.793, speed: 6334 char/sec\n",
      "62% ppl: 484.380, speed: 6454 char/sec\n",
      "72% ppl: 482.564, speed: 6539 char/sec\n",
      "82% ppl: 480.529, speed: 6375 char/sec\n",
      "92% ppl: 479.719, speed: 6349 char/sec\n",
      "Epoch: 22 Train Perplexity: 479.645\n",
      "Epoch: 23 Learning rate: 0.341\n",
      "3% ppl: 484.989, speed: 6724 char/sec\n",
      "13% ppl: 473.276, speed: 6245 char/sec\n",
      "23% ppl: 480.921, speed: 6629 char/sec\n",
      "33% ppl: 479.841, speed: 6821 char/sec\n",
      "43% ppl: 480.684, speed: 6880 char/sec\n",
      "52% ppl: 478.002, speed: 6907 char/sec\n",
      "62% ppl: 474.631, speed: 6955 char/sec\n",
      "72% ppl: 472.600, speed: 6994 char/sec\n",
      "82% ppl: 470.836, speed: 7005 char/sec\n",
      "92% ppl: 469.950, speed: 6710 char/sec\n",
      "Epoch: 23 Train Perplexity: 470.018\n",
      "Epoch: 24 Learning rate: 0.324\n",
      "3% ppl: 475.635, speed: 6042 char/sec\n",
      "13% ppl: 463.162, speed: 6871 char/sec\n",
      "23% ppl: 470.679, speed: 6988 char/sec\n",
      "33% ppl: 469.432, speed: 6889 char/sec\n",
      "43% ppl: 470.075, speed: 6757 char/sec\n",
      "52% ppl: 467.300, speed: 6818 char/sec\n",
      "62% ppl: 464.495, speed: 6860 char/sec\n",
      "72% ppl: 462.679, speed: 6573 char/sec\n",
      "82% ppl: 460.960, speed: 6542 char/sec\n",
      "92% ppl: 460.257, speed: 6465 char/sec\n",
      "Epoch: 24 Train Perplexity: 459.987\n",
      "Epoch: 25 Learning rate: 0.307\n",
      "3% ppl: 467.848, speed: 4788 char/sec\n",
      "13% ppl: 456.218, speed: 5543 char/sec\n",
      "23% ppl: 464.004, speed: 5502 char/sec\n",
      "33% ppl: 462.445, speed: 5178 char/sec\n",
      "43% ppl: 462.711, speed: 5487 char/sec\n",
      "52% ppl: 459.552, speed: 5613 char/sec\n",
      "62% ppl: 456.399, speed: 5727 char/sec\n",
      "72% ppl: 454.563, speed: 5846 char/sec\n",
      "82% ppl: 452.697, speed: 5999 char/sec\n",
      "92% ppl: 451.830, speed: 6106 char/sec\n",
      "Epoch: 25 Train Perplexity: 451.647\n",
      "Epoch: 26 Learning rate: 0.292\n",
      "3% ppl: 462.612, speed: 7060 char/sec\n",
      "13% ppl: 447.728, speed: 7414 char/sec\n",
      "23% ppl: 455.692, speed: 7356 char/sec\n",
      "33% ppl: 454.221, speed: 7355 char/sec\n",
      "43% ppl: 454.711, speed: 7324 char/sec\n",
      "52% ppl: 451.960, speed: 7357 char/sec\n",
      "62% ppl: 449.049, speed: 7365 char/sec\n",
      "72% ppl: 447.293, speed: 7373 char/sec\n",
      "82% ppl: 445.621, speed: 7375 char/sec\n",
      "92% ppl: 444.692, speed: 7317 char/sec\n",
      "INFO:tensorflow:Saving checkpoints for 7417 into logs/model.ckpt.\n",
      "Epoch: 26 Train Perplexity: 444.523\n",
      "Epoch: 27 Learning rate: 0.277\n",
      "3% ppl: 449.972, speed: 7025 char/sec\n",
      "13% ppl: 440.318, speed: 6877 char/sec\n",
      "23% ppl: 448.109, speed: 6916 char/sec\n",
      "33% ppl: 446.735, speed: 7024 char/sec\n",
      "43% ppl: 447.023, speed: 6506 char/sec\n",
      "52% ppl: 444.206, speed: 5540 char/sec\n",
      "62% ppl: 441.497, speed: 5447 char/sec\n",
      "72% ppl: 439.609, speed: 5646 char/sec\n",
      "82% ppl: 438.033, speed: 5734 char/sec\n",
      "92% ppl: 437.074, speed: 5620 char/sec\n",
      "Epoch: 27 Train Perplexity: 436.880\n",
      "Epoch: 28 Learning rate: 0.264\n",
      "3% ppl: 442.312, speed: 5947 char/sec\n",
      "13% ppl: 432.539, speed: 5956 char/sec\n",
      "23% ppl: 440.999, speed: 6372 char/sec\n",
      "33% ppl: 439.737, speed: 6234 char/sec\n",
      "43% ppl: 440.010, speed: 6256 char/sec\n",
      "52% ppl: 437.003, speed: 6395 char/sec\n",
      "62% ppl: 434.359, speed: 6498 char/sec\n",
      "72% ppl: 432.341, speed: 6570 char/sec\n",
      "82% ppl: 430.504, speed: 6659 char/sec\n",
      "92% ppl: 429.862, speed: 6693 char/sec\n",
      "Epoch: 28 Train Perplexity: 429.716\n",
      "Epoch: 29 Learning rate: 0.250\n",
      "3% ppl: 434.292, speed: 7529 char/sec\n",
      "13% ppl: 426.148, speed: 6754 char/sec\n",
      "23% ppl: 433.211, speed: 6233 char/sec\n",
      "33% ppl: 432.080, speed: 6172 char/sec\n",
      "43% ppl: 432.833, speed: 6316 char/sec\n",
      "52% ppl: 430.097, speed: 6104 char/sec\n",
      "62% ppl: 426.950, speed: 6057 char/sec\n",
      "72% ppl: 425.140, speed: 6193 char/sec\n",
      "82% ppl: 423.620, speed: 5926 char/sec\n",
      "92% ppl: 422.633, speed: 6013 char/sec\n",
      "Epoch: 29 Train Perplexity: 422.543\n",
      "Epoch: 30 Learning rate: 0.238\n",
      "3% ppl: 426.856, speed: 6950 char/sec\n",
      "13% ppl: 421.290, speed: 6129 char/sec\n",
      "23% ppl: 428.648, speed: 6416 char/sec\n",
      "33% ppl: 426.984, speed: 6576 char/sec\n",
      "43% ppl: 427.242, speed: 6640 char/sec\n",
      "52% ppl: 423.932, speed: 6754 char/sec\n",
      "62% ppl: 421.115, speed: 6694 char/sec\n",
      "72% ppl: 419.428, speed: 6352 char/sec\n",
      "82% ppl: 417.634, speed: 6409 char/sec\n",
      "92% ppl: 417.224, speed: 5934 char/sec\n",
      "Epoch: 30 Train Perplexity: 417.191\n",
      "Epoch: 31 Learning rate: 0.226\n",
      "3% ppl: 423.577, speed: 3162 char/sec\n",
      "13% ppl: 416.516, speed: 3376 char/sec\n",
      "23% ppl: 423.042, speed: 3621 char/sec\n",
      "33% ppl: 421.654, speed: 3760 char/sec\n",
      "43% ppl: 421.624, speed: 4039 char/sec\n",
      "52% ppl: 418.710, speed: 4390 char/sec\n",
      "62% ppl: 416.041, speed: 4692 char/sec\n",
      "72% ppl: 413.945, speed: 4940 char/sec\n",
      "82% ppl: 412.209, speed: 5136 char/sec\n",
      "92% ppl: 411.808, speed: 5305 char/sec\n",
      "Epoch: 31 Train Perplexity: 411.641\n",
      "Epoch: 32 Learning rate: 0.215\n",
      "3% ppl: 416.047, speed: 7607 char/sec\n",
      "13% ppl: 409.074, speed: 4906 char/sec\n",
      "23% ppl: 416.027, speed: 5503 char/sec\n",
      "33% ppl: 415.293, speed: 5969 char/sec\n",
      "43% ppl: 415.487, speed: 6216 char/sec\n",
      "52% ppl: 413.128, speed: 6383 char/sec\n",
      "62% ppl: 410.335, speed: 6074 char/sec\n",
      "72% ppl: 408.293, speed: 5785 char/sec\n",
      "82% ppl: 406.438, speed: 5711 char/sec\n",
      "92% ppl: 405.860, speed: 5806 char/sec\n",
      "Epoch: 32 Train Perplexity: 405.834\n",
      "Epoch: 33 Learning rate: 0.204\n",
      "3% ppl: 413.065, speed: 6616 char/sec\n",
      "13% ppl: 405.013, speed: 6511 char/sec\n",
      "23% ppl: 412.198, speed: 6721 char/sec\n",
      "33% ppl: 410.866, speed: 6898 char/sec\n",
      "43% ppl: 410.760, speed: 6945 char/sec\n",
      "52% ppl: 408.368, speed: 7016 char/sec\n",
      "62% ppl: 405.839, speed: 6998 char/sec\n",
      "72% ppl: 404.348, speed: 7049 char/sec\n",
      "82% ppl: 402.863, speed: 6994 char/sec\n",
      "92% ppl: 402.148, speed: 7007 char/sec\n",
      "Epoch: 33 Train Perplexity: 401.809\n",
      "Epoch: 34 Learning rate: 0.194\n",
      "3% ppl: 410.130, speed: 6837 char/sec\n",
      "13% ppl: 400.929, speed: 6755 char/sec\n",
      "23% ppl: 407.197, speed: 5547 char/sec\n",
      "33% ppl: 405.901, speed: 5405 char/sec\n",
      "43% ppl: 405.905, speed: 5554 char/sec\n",
      "52% ppl: 403.463, speed: 5610 char/sec\n",
      "62% ppl: 400.962, speed: 5307 char/sec\n",
      "72% ppl: 399.319, speed: 5464 char/sec\n",
      "82% ppl: 397.807, speed: 5609 char/sec\n",
      "92% ppl: 397.323, speed: 5727 char/sec\n",
      "Epoch: 34 Train Perplexity: 397.150\n",
      "Epoch: 35 Learning rate: 0.184\n",
      "3% ppl: 400.391, speed: 6776 char/sec\n",
      "13% ppl: 394.661, speed: 6947 char/sec\n",
      "23% ppl: 401.625, speed: 6960 char/sec\n",
      "33% ppl: 400.674, speed: 6993 char/sec\n",
      "43% ppl: 400.999, speed: 6999 char/sec\n",
      "52% ppl: 398.787, speed: 7000 char/sec\n",
      "62% ppl: 396.481, speed: 7017 char/sec\n",
      "72% ppl: 394.693, speed: 6680 char/sec\n",
      "82% ppl: 393.086, speed: 6015 char/sec\n",
      "92% ppl: 392.645, speed: 5830 char/sec\n",
      "Epoch: 35 Train Perplexity: 392.612\n",
      "Epoch: 36 Learning rate: 0.175\n",
      "3% ppl: 397.895, speed: 5719 char/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13% ppl: 391.634, speed: 6525 char/sec\n",
      "23% ppl: 397.902, speed: 6821 char/sec\n",
      "33% ppl: 396.445, speed: 6484 char/sec\n",
      "43% ppl: 396.905, speed: 5848 char/sec\n",
      "52% ppl: 394.781, speed: 6077 char/sec\n",
      "62% ppl: 392.516, speed: 6174 char/sec\n",
      "72% ppl: 390.825, speed: 6246 char/sec\n",
      "82% ppl: 389.389, speed: 6155 char/sec\n",
      "92% ppl: 389.069, speed: 6045 char/sec\n",
      "Epoch: 36 Train Perplexity: 388.843\n",
      "Epoch: 37 Learning rate: 0.166\n",
      "3% ppl: 397.144, speed: 7265 char/sec\n",
      "13% ppl: 388.068, speed: 7253 char/sec\n",
      "23% ppl: 393.321, speed: 7204 char/sec\n",
      "33% ppl: 392.376, speed: 7044 char/sec\n",
      "43% ppl: 393.071, speed: 6708 char/sec\n",
      "52% ppl: 390.835, speed: 6531 char/sec\n",
      "62% ppl: 388.354, speed: 6623 char/sec\n",
      "72% ppl: 386.999, speed: 6548 char/sec\n",
      "82% ppl: 385.507, speed: 6484 char/sec\n",
      "92% ppl: 384.823, speed: 6551 char/sec\n",
      "Epoch: 37 Train Perplexity: 384.504\n",
      "Epoch: 38 Learning rate: 0.158\n",
      "3% ppl: 391.009, speed: 7058 char/sec\n",
      "13% ppl: 383.321, speed: 6928 char/sec\n",
      "23% ppl: 389.161, speed: 7077 char/sec\n",
      "33% ppl: 388.315, speed: 7112 char/sec\n",
      "43% ppl: 388.626, speed: 7115 char/sec\n",
      "52% ppl: 386.226, speed: 7101 char/sec\n",
      "62% ppl: 384.345, speed: 7090 char/sec\n",
      "72% ppl: 382.553, speed: 7128 char/sec\n",
      "82% ppl: 381.318, speed: 7148 char/sec\n",
      "92% ppl: 380.758, speed: 7162 char/sec\n",
      "Epoch: 38 Train Perplexity: 380.717\n",
      "Epoch: 39 Learning rate: 0.150\n",
      "3% ppl: 389.936, speed: 5688 char/sec\n",
      "13% ppl: 382.106, speed: 5490 char/sec\n",
      "23% ppl: 388.288, speed: 6084 char/sec\n",
      "33% ppl: 386.513, speed: 6137 char/sec\n",
      "43% ppl: 386.711, speed: 5869 char/sec\n",
      "52% ppl: 384.466, speed: 6120 char/sec\n",
      "62% ppl: 382.064, speed: 6029 char/sec\n",
      "INFO:tensorflow:Saving checkpoints for 11058 into logs/model.ckpt.\n",
      "72% ppl: 380.414, speed: 5941 char/sec\n",
      "82% ppl: 378.742, speed: 5857 char/sec\n",
      "92% ppl: 378.355, speed: 5772 char/sec\n",
      "Epoch: 39 Train Perplexity: 378.240\n",
      "Epoch: 40 Learning rate: 0.142\n",
      "3% ppl: 382.897, speed: 6259 char/sec\n",
      "13% ppl: 375.810, speed: 6853 char/sec\n",
      "23% ppl: 382.443, speed: 6712 char/sec\n",
      "33% ppl: 381.199, speed: 6502 char/sec\n",
      "43% ppl: 381.921, speed: 6601 char/sec\n",
      "52% ppl: 379.756, speed: 6441 char/sec\n",
      "62% ppl: 377.530, speed: 6470 char/sec\n",
      "72% ppl: 375.878, speed: 6504 char/sec\n",
      "82% ppl: 374.418, speed: 6563 char/sec\n",
      "92% ppl: 374.302, speed: 6611 char/sec\n",
      "Epoch: 40 Train Perplexity: 374.284\n",
      "Test Perplexity: 310.655\n",
      "君马十年年\n",
      "时中君安一\n",
      "里不前既不\n",
      "龙前食上风\n",
      "\n",
      "INFO:tensorflow:Saving checkpoints for 11440 into logs/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgU5M-MXVB9x"
   },
   "source": [
    "We can see the training process shown here. Observe that training loss keeps decreasing, which means that the model is actually learning. \n",
    "\n",
    "You can also continue training by calling the controller again. Try this later and see if the poems generated gets better over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW284a7xioaw"
   },
   "source": [
    "## Clear previous output\n",
    "\n",
    "Tensorflow will automatically load previous models if you specify a path for the `session`. However, that will be a problem if you change some parts of the model. e.g., change embedding size, LSTM size, or number of layers.\n",
    "\n",
    "You will see something like \n",
    "```\n",
    "INFO:tensorflow:Restoring parameters from logs/model.ckpt-4465\n",
    "...\n",
    "InvalidArgumentError: Assign requires shapes of both tensors to match.\n",
    "```\n",
    "Always remember to clear output directory if you are experimenting with different model structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfkNbkgyG81Z"
   },
   "outputs": [],
   "source": [
    "!rm -R logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWfT6DruhUBd"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week11_NLP_codelab_Seq2seq",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
